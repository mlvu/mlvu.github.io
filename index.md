---
layout: default
title: Machine Learning @ VU
---
{::nomarkdown}
<nav class="lectures">

<div class="l1"><a href="/lecture01">
 <h3 class="red"><span>1.</span> Introduction</h3>
</a>
</div>

<div class="l2"><a href="/lecture02">
 <h3 class="orange"><span>2.</span> Linear models and search</h3>
</a>
</div>

<div class="l3"><a href="/lecture03">
 <h3 class="green"><span>3.</span> Evaluation</h3>
</a>
</div>


<div class="l4"><a href="/lecture04">
 <h3 class="blue"><span>4.</span>Probabilistic models</h3>
</a>
</div>

<div class="l5"><a href="/lecture05">
 <h3 class="green"><span>5.</span>Data pre-processing</h3>
</a>
</div>

<div class="l6"><a href="/lecture06">
 <h3 class="green"><span>6.</span>Beyond linear models</h3>
</a>
</div>


</nav>
{:/nomarkdown}

This page contains all public information about the course _Machine Learning_ at the VU University Amsterdam. We provide the following materials:
  * **Lecture slides and videos.**
  * **Worksheets** These are very brief Jupyter notebooks to help you get the software installed and to show the basics. They introduce the libraries Numpy, Matplotlib, Pandas, Sklearn and Keras.
  * **Homework** The homework consists of small pen-and-paper exercises to help you test that you've really understood the more technical points of the lectures. Answers are provided.
If you are a registered student, please refer to the Canvas page instead. All material authored by [Peter Bloem](http://peterbloem.nl/) unless noted otherwise.

Reuse is allowed under a creative commons license, [details below](#keynote-files-and-re-use-license).

## All content

{::nomarkdown}
<table class="overview">
	<colgroup>
		<col class="week">
		<col class="lecture">
		<col class="links">
		<col class="homework">
		<col class="worksheets">
		<col class="previous">
	</colgroup>

  <tr>
    <th></th>
    <th></th>
    <th></th>
    <th>homework</th>
    <th>worksheets</th>
    <th>previous</th>
  </tr>
  
  <tr>
   <td rowspan="2">w1</td> 
   <td>
	   <h3>1. Introduction </h3>
	   <ul class="videos">
		 <li><a href="https://youtu.be/G5vMe_A5OTo">1.1 What is machine learning?</a></li>
		 <li><a href="https://youtu.be/NPqYy-OuNEA">1.2 Classification</a></li>
		 <li><a href="https://youtu.be/pJIrdzhsWRM">1.3 Other abstract tasks</a></li>
		 <li><a href="https://youtu.be/uWmbIQHtjhk">1.4 Social impact 1</a></li>
		 <li><a href="https://youtu.be/24fPK3CM-8E">1.5 Generalization</a></li>
	   </ul>
   </td>
   
   <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgv6tP8gfDUGqpkBn8u5c5PY">playlist</a> <a href="./lectures/11.Introduction.annotated.pdf">slides</a> 
   <td rowspan="2"><a href="./homework/week1.noanswers.pdf">plain</a>, <a href="./homework/week1.answers.pdf">answers</a></td> 
   <td rowspan="2"><a href="https://docs.google.com/document/d/1-LXG5Lb76xQy70W2ZdannnYMEXRLt0CsoiaK0gTkmfY/edit">getting set up</a>, <a href="https://github.com/mlvu/worksheets/blob/master/Worksheet%201%2C%20Numpy%20and%20Matplotlib.ipynb">numpy</a></td> 
   <td><a href="https://youtu.be/excCZSTJEPs">2020</a> <a href="https://youtu.be/-pve3oIvxa8">2019</a> <a href="https://youtu.be/547GyRCr8TM">2018</a></td>
</tr>
  
  <tr>
  	<td><h3>2. Linear models and search </h3>
  	<ul class="videos">
		 <li><a href="https://youtu.be/YGBZE6RA7aU">2.1 Linear regression</a></li>
		 <li><a href="https://youtu.be/q97nOAYfpHg">2.2 Searching for a good model</a></li>
		 <li><a href="https://youtu.be/DlMKkPrKg5c">2.3 Gradient descent</a></li>
		 <li><a href="https://youtu.be/LjhzJUy-wfk">2.4 Linear classification</a></li>
	   </ul>
  	</td>
    <td> <a href="https://www.youtube.com/playlist?list=PLCof9EqayQguePOyoKenR5LWOlCWCkCsM">playlist</a> <a href="./lectures/12.LinearModels1.annotated.pdf">slides</a> </td>
	<td> <a href="https://youtu.be/1lqaD0AsMfY">2020</a> <a href="https://youtu.be/3K4pNmQbGx8">2019</a> <a href="https://youtu.be/sSykYt7H8oE">2018</a></td>
  </tr>
  
  <tr>
    <td rowspan="2"> w2</td>
    <td><h3>3. Model evaluation</h3>
    <ul class="videos">
		 <li><a href="https://youtu.be/hHLDDJJl2v4">3.1 Machine learning experiments</a></li>
		 <li><a href="https://youtu.be/oZdWt_Mrg_8">3.2 Model evaluation</a></li>
		 <li><a href="https://youtu.be/UrqPyE4H2bI">3.3 ROC curves, area under the curve</a></li>
		 <li><a href="https://youtu.be/XJ_StAFefUk">3.4 Social impact 2</a></li>  
		 <li><a href="https://youtu.be/BfoeXjX2v0Q">3.5 Statistics for ML experiments</a></li>
		 <li><a href="https://youtu.be/gnmCkRYrlEA">3.6 No free lunch</a></li>  		   
    </ul>
    
    </td>
    <td> <a href="https://www.youtube.com/playlist?list=PLCof9EqayQgt6iSJnt8ABPhMNiU2hmZiK">playlist</a> <a href="./lectures/21.Methodology1.annotated.pdf">slides</a></td>
    <td rowspan="2"><a href="./homework/week2.noanswers.pdf">plain</a>, <a href="./homework/week2.answers.pdf">answers</a></td>
    <td rowspan="2"><a href="https://github.com/mlvu/worksheets/blob/master/Worksheet%202%2C%20Sklearn.ipynb">sklearn</a></td>
    <td><a href="https://youtu.be/GaoNNaRjauU">2020</a> <a href="https://youtu.be/wbUE6X_B8B4">2019</a> <a href="https://youtu.be/okxskT6ben4">2018</a></td>
  </tr>
  <tr>
    <td><h3>4. Data pre-processing</h3>
    <ul class="videos">
		 <li><a href="https://youtu.be/hHLDDJJl2v4">4.1 Missing values and outliers</a></li>
		 <li><a href="https://youtu.be/oZdWt_Mrg_8">4.2 Class imbalance and feature design</a></li>
		 <li><a href="https://youtu.be/UrqPyE4H2bI">4.3 Normalization</a></li>
		 <li><a href="https://youtu.be/XJ_StAFefUk">4.4 Principal component analysis</a></li>  		   
    </ul>    
    
    </td>
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgsJDHLYb0dxn5xr2-pjmV08">playlist</a> <a href="./lectures/22.Methodology2.annotated.pdf">slides</a> </td> 
	<td><a href="https://youtu.be/Aad5UDrdHPg">2020</a> <a href="https://youtu.be/H4c4qpHdGq8">2019</a> <a href="https://youtu.be/csk2HSWS5r8">2018</a></td>
  </tr>
  <tr>
    <td rowspan="2"> w3</td>
    <td><h3> 5. Probabilistic Models</h3>
	<ul class="videos">
		 <li><a href="https://youtu.be/9rWoBVnVuLQ">5.1 Introduction to probability</a></li>
		 <li><a href="https://youtu.be/IubHHpzM32Y">5.2 Learning with probability</a></li>
		 <li><a href="https://youtu.be/fK6dQYkeVqA">5.3 The (naive) Bayes classifier</a></li>
		 <li><a href="https://youtu.be/EYhxR22Ta88">5.4 Logistic regression</a></li>  		   
		 <li><a href="https://youtu.be/mSneVjDvzNQ">5.5 Information theory</a></li>  		   
    </ul>      
    </td>

    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgs6gGKPUDURn6aoV6gCIUPa">playlist</a> <a href="./lectures/31.ProbabilisticModels1.annotated.pdf">slides</a>  </td>
    <td rowspan="2"><a href="./homework/week3.noanswers.pdf">plain</a>, <a href="./homework/week3.answers.pdf">answers</a></td>
    <td rowspan="2"><a href="https://github.com/mlvu/worksheets/blob/master/Worksheet%203%2C%20Pandas.ipynb">pandas</a></td>
	<td><a href="https://youtu.be/k0_56JyYaOM">2020</a> <a href="https://youtu.be/f2HIW37Ohho">2019</a> <a href="https://youtu.be/DM1APCpqF8g">2018</a></td>
  </tr>
  <tr>
    <td><h3>6. Beyond Linear models</h3>
	<ul class="videos">
         <li><a href="https://youtu.be/DeQ4STHYT3g">6.1 Neural networks</a></li>
		 <li><a href="https://youtu.be/IZ4w-aG50nU">6.2 Backpropagation</a></li>
		 <li><a href="https://youtu.be/-PvsRdlISls">6.3 Support vector machines</a></li>
		 <li><a href="https://youtu.be/cPbsqPg-s2Y">6.4 Lagrange multipliers</a></li>  		   
		 <li><a href="https://youtu.be/rILXgY0IHxA">6.5 The kernel trick</a></li>  		 		   
    </ul>           
    </td>
    
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgs-MP4aQQ-2teemZANWKBjh">playlist</a> <a href="./lectures/32.LinearModels2.annotated.pdf">slides</a></td>
	<td><a href="https://youtu.be/1NVgspM98W0">2020</a> <a href="https://youtu.be/g2lziWxf_9Q">2019</a> <a href="https://youtu.be/F6gFYAwXmAs">2018</a></td>
 </tr>  
 
 <tr>
    <td rowspan="2"> w4</td>
    <td><h3>7. Deep Learning</h3>
    <ul class="videos">
         <li><a href="">7.1 Deep learning and automatic differentiation</a></li>
		 <li><a href="">7.2 Tensor backpropagation</a></li>
		 <li><a href="">7.3 Convolutions</a></li>
		 <li><a href="">7.4 Making it work</a></li>  		   
    </ul>
    
    </td>
    <td> <a href="https://www.youtube.com/playlist?list=PLCof9EqayQgvCGzTPoRXPEYUWvFl8Cj71">playlist</a> <a href="./lectures/41.DeepLearning1.annotated.pdf">slides</a></td> 
    <td rowspan="2"><a href="./homework/week4.noanswers.pdf">plain</a>, <a href="./homework/week4.answers.pdf">answers</a></td> 
    <td rowspan="2"><a href="https://github.com/mlvu/worksheets/blob/master/Worksheet%204%2C%20Keras.ipynb">keras</a></td> 
	<td><a href="https://youtu.be/DidHjsp_OV0">2020</a> <a href="https://youtu.be/VZwrbIBNzzA">2019</a> <a href="https://youtu.be/jOrYBnEPpYU">2018</a></td>
  </tr>
  <tr>
    <td><h3>8. Density estimation</h3>
    
    <ul class="videos">
         <li><a href="https://youtu.be/VZfAJzXu1hM">8.1 Normal distributions</a></li>
		 <li><a href="https://youtu.be/lRuiHSxtb1w">8.2 Maximum likelihood estimators</a></li>
		 <li><a href="https://youtu.be/Co3xlK2d_oI">8.3 Expectation-maximization</a></li>
		 <li><a href="https://youtu.be/MFQdNuVCk4A">8.4 Expectation-maximization from first principles</a></li>  		   
		 <li><a href="https://youtu.be/r4DYGXmbk_E">8.5 Social impact 3</a></li>
	</ul>     

    </td>
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgt9mF2CMt3NI1SZQQ_NYe28">playlist</a> <a href="./lectures/42.ProbabilisticModels2.annotated.pdf">slides</a> </td>
	<td><a href="https://youtu.be/ZIX7PZgz4qs">2020</a> <a href="https://youtu.be/f2HIW37Ohho">2019</a> <a href="https://youtu.be/-7UJqvjNIjk">2018</a></td>
  </tr>
    <tr>
    <td rowspan="2"> w5</td>
    <td><h3>9. Deep generative models</h3>
    
	<ul class="videos">
         <li><a href="https://youtu.be/jAxUolSXGtg">9.1 Generator networks</a></li>
		 <li><a href="https://youtu.be/eaWxDebDDo8">9.2 Generative adversarial networks</a></li>
		 <li><a href="https://youtu.be/t6GxDo1fSt0">9.3 Autoencoders</a></li>
		 <li><a href="https://youtu.be/inUJd7f931g">9.4 Variational autoencoders</a></li>  		   
	</ul>    
	
    </td>
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQguK56djSLj2C4LHNfLVZGjg">playlist</a> <a href="./lectures/51.Deep Learning2.annotated.pdf">slides</a> </td> 
    <td rowspan="2"><a href="./homework/week5.noanswers.pdf">plain</a>, <a href="./homework/week5.answers.pdf">answers</a></td> 
    <td rowspan="2"><a href="https://github.com/mlvu/worksheets/blob/master/Worksheet%205%2C%20Pytorch.ipynb">pytorch</a></td> 
	<td><a href="https://youtu.be/0zTkHTk_-6s">2020</a> <a href="https://youtu.be/6N4zIx0ATME">2019</a> <a href="https://youtu.be/APuP9SkESGA">2018</a></td>
  </tr>
  <tr>
    <td><h3>10. Tree Model and Ensembles</h3>
  	<ul class="videos">
         <li><a href="https://youtu.be/1JxBgetslSY">10.1 Decision trees</a></li>
		 <li><a href="https://youtu.be/y6pHc1iB6a0">10.2 Regression trees and numeric features</a></li>
		 <li><a href="https://youtu.be/9ikKZYxsfbg">10.3 Ensembling</a></li>
		 <li><a href="https://youtu.be/mSLRpkynW9Y">10.4 Boosting</a></li>  		   
	</ul>    
    
    </td>
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgv5bPKEn7F1AEMyojCgTibW">playlist</a> <a href="./lectures/52.Trees.annotated.pdf">slides</a> </td> 
	<td><a href="https://youtu.be/3pkOMubnwA8">2020</a> <a href="https://youtu.be/m-at5l3F_ig">2019</a> <a href="https://youtu.be/PGITM1E2CLk">2018</a></td>
  </tr>
  <tr>
    <td rowspan="2"> w6</td>
    <td><h3>11. Models for Sequential Data</h3>
    <ul class="videos">
    	 <li><a href="https://youtu.be/wf8D0QWe0hg">11.1 Markov models</a></li>
		 <li><a href="https://youtu.be/mnkJSiS3ooc">11.2 Deep learning on sequences</a></li>
		 <li><a href="https://youtu.be/KUjsy7Hp8fE">11.3 Recurrent neural nets and LSTMs</a></li>
	</ul>
    </td>
    <td> <a href="https://www.youtube.com/playlist?list=PLCof9EqayQgtsKcp3029eKm6mzwxNbzxV">playlist</a> <a href="./lectures/61.SequentialModels.annotated.pdf">slides</a> </td> 
    <td rowspan="2"><a href="./homework/week6.noanswers.pdf">plain</a> <a href="./homework/week6.answers.pdf">answers</a></td> 
    <td rowspan="2"></td> 
	<td><a href="https://youtu.be/T2txIYwRPDo">2020</a>  <a href="https://youtu.be/h6j9wgHGnOk">2019</a> <a href="https://youtu.be/HNOHLvD6_gs">2018</a></td>
  </tr>
  <tr>
    <td><h3>12. Embedding models</h3>
    <ul class="videos">
    	 <li><a href="https://youtu.be/q_s2DLcV284">12.1 Recommender systems</a></li>
		 <li><a href="https://youtu.be/q_s2DLcV284">12.2 Improving recommender systems</a></li>
		 <li><a href="https://youtu.be/ww6bYts4yOs">12.3 PCA revisited</a></li>
 		 <li><a href="https://youtu.be/2mh3sAo9iC0">12.4 Graph models</a></li>
 		 <li><a href="https://youtu.be/1ALoFBlAaUk">12.5 Validation of embedding models</a></li>
	</ul>    
    </td>
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgv8gcTly0uC7gXiHjPR_c1_">playlist</a> <a href="./lectures/62.Matrices.annotated.pdf">slides</a></td>
    <td> <a href="https://youtu.be/TwCYexIpqUU">2020</a> <a href="https://youtu.be/L2mJ4o7F434">2019</a> <a href="https://youtu.be/RByePOW2b1c">2018</a></td>
  </tr>
  <tr>
    <td rowspan="2"> w7</td>
    <td><h3>13. Reinforcement Learning</h3>
    <ul class="videos">
    	 <li><a href="https://youtu.be/EEqO13EKuFg">13.1 Reinforcement learning</a></li>
		 <li><a href="https://youtu.be/tbV2udJmssM">13.2 Policy gradients and random search</a></li>
		 <li><a href="https://youtu.be/xf_MHXxzf1Y">13.3 (Deep) Q earning</a></li>
 		 <li><a href="https://youtu.be/R4souHAdRP4">13.4 AlphaGo</a></li>
		 <li><a href="https://youtu.be/fh2aozmj2F4">13.5 social impact 4</a></li>
	</ul>        
    </td>
    <td><a href="https://www.youtube.com/playlist?list=PLCof9EqayQgsfoBHwl0df2o5RXeKAcYPd">playlist</a> <a href="./lectures/71.Reinforcement Learning.annotated.pdf">slides</a> </td> 
    <td rowspan="2"></td> 
    <td rowspan="2"></td>
	<td> <a href="https://youtu.be/2ruGrJb8Glk">2019</a> <a href="https://youtu.be/RByePOW2b1c">2018</a></td>
  </tr>
  <tr>
    <td><h3>14. Review <h3></td>
    <td><a href="https://youtu.be/xMNc8T_W0Ks">video</a> <a href="./lectures/72.Review.annotated.pdf">slides</a> </td>    
	<td><a href="https://youtu.be/xMNc8T_W0Ks">2019</a> <a href="https://youtu.be/sMd9ReLSSYU">2018</a></td>
  </tr>
  <tr>
    <td> w8</td><td colspan="6">Exam. See below for practice exams.</td>
  </tr>
  
</table>
{:/nomarkdown}

Feel free to open a [github issue](https://github.com/mlvu/mlvu.github.io/issues) if you're 
working through the material and you spot a mistake, run into a problem or have any other
 kind of question. We also try to answer questions on youtube.

## Required reading

Each week comes with a small amount of reading material. Most resources are publicly 
available free of charge. If you are a VU student, check Canvas for PDFs of the 
copyrighted works.

<table>
<tr>
  <td>Week 1</td>
  <td>Deep Learning, Goodfellow et al. <a href="https://www.deeplearningbook.org/contents/ml.html">Section 5.1</a></td>
</tr>
<tr>
  <td>Week 2</td>
  <td>
  Machine Learning, Peter Flach. Section 2.2<br/>
  <a href="http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/">Everything you did and didn't know about PCA</a>, Alex Williams
  </td>
</tr>
<tr>
  <td>Week 3</td>  
 <td>Neural Networks and Deep Learning, <a href="http://neuralnetworksanddeeplearning.com/chap6.html">Chapter 6</a></td>
</tr>
<tr>
  <td>Week 4</td>  
 <td><a href="https://datajobs.com/data-science-repo/Expectation-Maximization-Primer-[Do-and-Batzoglou].pdf">What is the expectation maximization algorithm?</a> Do et al.</td>
</tr>
<tr>
  <td>Week 5</td>
  <td><a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">Intuitively Understanding Variational Autoencoders</a>, Irhum Shafkat<br/>
  Machine Learning, Tom Mitchell. Chapter 3.
  </td>
</tr>
<tr>
  <td>Week 6</td>
 <td><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">Understanding LSTM Networks</a>, Chris Olah </td>
</tr>
<tr>
  <td>Week 7</td>
 <td><a href="http://karpathy.github.io/2016/05/31/rl/">Reinforcement Learning: Pong from pixels</a>, Andrej Karpathy</td>
</tr>
</table>

## Practice exams

Each exam consists of 40 multiple choice questions.

* <a href="./exams/exam2018.noanswers.pdf">Final exam 2018</a>, <a href="./exams/exam2018.answers.pdf">answers</a>
* <a href="./exams/resit2018.noanswers.pdf">Resit 2018</a>, <a href="./exams/resit2018.answers.pdf">answers</a>

## Keynote files and re-use license

All material that is original to this course may be used under a [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license. That means you are free to use the material, and adapt it, so long as appropriate credit is given. You may redistribute only under the same license.

**How to credit:** 
* For individual slides, please add a link to mlvu.github.io, on the slide, or in the published slide annotations. 
* If you are using a slide deck for a lecture as is, please indicate the source of the slides as mlvu.github.io clearly at the start of the lecture. Leaving the existing URL in place on the opening slide suffices.
* If you use many of the slides, a single attribution can be made once at the start of the slide deck. 
Crediting me by name ([Peter Bloem](http://peterbloem.nl/)) is appreciated, but not strictly necessary.

If you would like to use the material, but do not want to attribute in this way for some reason, please get in touch. I'm sure we can work something out.

Some parts of the material are taken from other sources. The source should always be credited on the slide itself (let me know if this isn't the case). Please adapt and redistribute these only under the original licenses.

### Keynote files

The original keynote files for the lectures (of the 2019 version) [can be found here](https://github.com/mlvu/mlvu.github.io/tree/master/lectures-keynote), and may be used under the terms of the license above. These can be converted to ppt, but the formulas may not survive the conversion process. 

The formulas were typeset using a fantastic tool called [LaTeXiT](https://www.chachatelier.fr/latexit/). Copy pasting a formula from Keynote to LaTeXiT should reveal the original LaTeX. You'll need to use [this preample]() for the typesetting to work.




